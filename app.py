import streamlit as st
st.set_page_config(page_title="Real-time Emotion Recognition", layout="centered")

import cv2
import torch
import torch.nn as nn
import numpy as np
from torchvision import transforms, models
from PIL import Image
from collections import Counter
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase
import av
import gdown
import os
import matplotlib.pyplot as plt

# ==== –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã ====
MODEL_PATH = "best_emotion_model_gray48_1.pth"
FILE_ID = "1h6OZWxlWDr_IDzlb4LucQzWV56kSqgza"
DOWNLOAD_URL = f"https://drive.google.com/uc?id={FILE_ID}"
class_names = ['angry', 'fear', 'happy', 'neutral', 'sad', 'surprise']
positive_emotions = ['happy', 'neutral', 'surprise']
emotion_log = []

# ==== –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ ====
@st.cache_resource
def load_model():
    if not os.path.exists(MODEL_PATH):
        with st.spinner("‚¨áÔ∏è –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å Google Drive..."):
            gdown.download(DOWNLOAD_URL, MODEL_PATH, quiet=False)

    model = models.resnet18()
    model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)
    model.fc = nn.Linear(model.fc.in_features, 6)
    model.load_state_dict(torch.load(MODEL_PATH, map_location="cpu"))
    model.eval()
    return model

model = load_model()

# ==== –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ ====
transform = transforms.Compose([
    transforms.Resize((48, 48)),
    transforms.Grayscale(num_output_channels=1),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
])

face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")

# ==== –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –≤–∏–¥–µ–æ ====
class EmotionProcessor(VideoProcessorBase):
    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        img = frame.to_ndarray(format="bgr24")
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(gray, 1.3, 5)

        for (x, y, w, h) in faces:
            face_img = gray[y:y+h, x:x+w]
            face_pil = Image.fromarray(face_img).convert("L")
            input_tensor = transform(face_pil).unsqueeze(0)

            with torch.no_grad():
                output = model(input_tensor)
                _, pred = torch.max(output, 1)
                predicted_emotion = class_names[pred.item()]
                emotion_log.append(predicted_emotion)

            # –û—Ç—Ä–∏—Å–æ–≤–∫–∞
            color = (0, 255, 0) if predicted_emotion in positive_emotions else (0, 0, 255)
            cv2.rectangle(img, (x, y), (x+w, y+h), color, 2)
            cv2.putText(img, predicted_emotion, (x, y-10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)

        return av.VideoFrame.from_ndarray(img, format="bgr24")

# ==== –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å ====
st.title("üé• Emotion Recognition with Camera")
st.markdown("–í–∫–ª—é—á–∏—Ç–µ –∫–∞–º–µ—Ä—É –∏ –Ω–∞–±–ª—é–¥–∞–π—Ç–µ –∑–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ–º —ç–º–æ—Ü–∏–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏.")

# –ö–∞–º–µ—Ä–∞
ctx = webrtc_streamer(key="emotion", video_processor_factory=EmotionProcessor)

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
if st.button("üìä –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"):
    if not emotion_log:
        st.warning("–ù–µ—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã—Ö —ç–º–æ—Ü–∏–π ‚Äî –≤–∫–ª—é—á–∏—Ç–µ –∫–∞–º–µ—Ä—É –∏ –ø–æ–¥–æ–∂–¥–∏—Ç–µ.")
    else:
        stats = Counter(emotion_log)
        st.markdown("### üìà –≠–º–æ—Ü–∏–∏ –∑–∞ —Å–µ—Å—Å–∏—é:")
        total = sum(stats.values())
        for emo, count in stats.items():
            percent = (count / total) * 100
            st.write(f"**{emo}** ‚Äî {count} —Ä–∞–∑ ({percent:.1f}%)")

        # –ì—Ä–∞—Ñ–∏–∫
        fig, ax = plt.subplots()
        ax.bar(stats.keys(), stats.values(), color='skyblue')
        ax.set_title("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —ç–º–æ—Ü–∏–π")
        st.pyplot(fig)

        # –í–µ—Ä–¥–∏–∫—Ç
        pos_total = sum(stats[e] for e in positive_emotions)
        ratio = pos_total / total
        st.markdown("### üßæ –ò—Ç–æ–≥:")
        if ratio >= 0.5:
            st.success("‚úÖ –ö–ª–∏–µ–Ω—Ç —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—ë–Ω")
        else:
            st.error("‚ùå –ö–ª–∏–µ–Ω—Ç –Ω–µ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—ë–Ω")
